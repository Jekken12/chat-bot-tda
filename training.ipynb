{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD  # Importación correcta de SGD\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545 documents\n",
      "161 classes [' dormir', 'acerca-de', 'algo-else', 'amigos', 'animo', 'ansioso', 'apoyo_emocional', 'aprender-mas', 'aprender-salud-mental', 'articulos_contenido', 'articulos_descripcion', 'articulos_objetivo', 'asustado', 'ayuda', 'casual', 'chiste_animales', 'chiste_de_comida', 'chiste_de_escuela', 'chiste_de_familia', 'chiste_de_tecnologia', 'chiste_de_viajes', 'chiste_general', 'chistes-serios-o', 'ciencia', 'citas', 'clima', 'cocina', 'comida', 'como_funciona_test', 'como_funcionan_retos', 'consejos_de_vida', 'consejos_estudio', 'contenido_video_tadh', 'creacion', 'crear_usuario', 'cuidado_personal', 'cultura', 'curiosidad_nombre_atenia', 'deprimido', 'descripcion_retosa', 'despedida', 'detalles_video_tadh', 'diario_como_usar', 'diario_descripcion', 'diario_objetivo', 'diario_requisitos', 'ejercicio', 'entender', 'entender_video_tadh', 'estilo_de_vida', 'estresado', 'estupido', 'eventos_actuales', 'eventos_futuros', 'explicacion_nombre_atenia', 'feliz', 'filosofia', 'finanzas', 'formulario_crear_cuenta', 'formulario_iniciar_sesion', 'gracias', 'habilidad', 'habitos_estudio', 'hecho', 'hecho-salud-mental', 'hobbies', 'hobbies_nuevos', 'impacto_tadh_video', 'incorrecto', 'informacion_integrantes', 'informacion_psicologo', 'informacion_universidad', 'informacion_video_tadh', 'iniciar_sesion', 'integrantes_equipo', 'inutil', 'jamila-util', 'juegos', 'libros', 'login_usuario', 'manana', 'mascotas', 'meditacion', 'moda', 'muerte', 'musica', 'no-hablar', 'noche', 'noticias', 'odio-a-mi', 'odio-a-ti', 'origen_nombre_atenia', 'origen_tadh_video', 'por-defecto', 'preguntar', 'problema', 'psicologo_aprueba_pagina', 'psicologo_validacion', 'que es la depresion?', 'quien_creo_web', 'recomendacion_actividades_relajantes', 'recomendacion_alimentacion', 'recomendacion_ambiente_trabajo', 'recomendacion_apoyo_social', 'recomendacion_autocuidado', 'recomendacion_comunicacion', 'recomendacion_diario_emocional', 'recomendacion_ejercicio', 'recomendacion_empatía', 'recomendacion_estrategias_autoayuda', 'recomendacion_estrategias_estrés', 'recomendacion_estrategias_tiempo', 'recomendacion_habitos_sueño', 'recomendacion_meditacion', 'recomendacion_organizacion', 'recomendacion_positividad', 'recomendacion_terapia', 'recomendacion_tiempo_fuera', 'recomendaciones_peliculas', 'recordatorio_descanso', 'recordatorio_hidratacion', 'registro_usuario', 'relaciones', 'repetir', 'respuesta-neutral', 'resumen_habitos', 'reto_1_beneficios', 'reto_1_descripcion', 'reto_1_detalles', 'reto_2_beneficios', 'reto_2_descripcion', 'reto_2_detalles', 'reto_3_beneficios', 'reto_3_descripcion', 'reto_3_detalles', 'reto_4_beneficios', 'reto_4_descripcion', 'reto_4_dificultad', 'reto_4_mecanica', 'reto_5_beneficios', 'reto_5_descripcion', 'reto_5_dificultad', 'reto_5_mecanica', 'retos', 'salud', 'saludos', 'significado_nombre_atenia', 'sin-enfoque', 'sin-respuesta', 'suicidio', 'tarde', 'tecnologia', 'tecnologia_nueva', 'test_resultado', 'test_tda', 'triste', 'ubicacion', 'usuario-acuerda', 'usuario-consejo', 'usuario-meditacion', 'viajes']\n",
      "624 unique lemmatized words [',', '.', '1', '2', '3', '4', '5', '[', ']', 'a', 'abajo', 'abrirme', 'acercan', 'actitud', 'actividades', 'activo', 'adecuado', 'adios', 'administrar', 'afecta', 'agotado', 'agradezco', 'agua', 'ahi', 'ahora', 'ahorrar', 'al', 'alegre', 'alejate', 'algo', 'alguien', 'alguna', 'algún', 'alimentación', 'alimentos', 'ambiente', 'amigo', 'animal', 'animales', 'animarme', 'ansioso', 'aparece', 'aparecen', 'apoyame', 'apoyo', 'aprender', 'aprobación', 'aprobar', 'aprobó', 'aprueba', 'artificial', 'artículos', 'asi', 'atención', 'atenia', 'atrapado', 'au', 'autoayuda', 'autocuidado', 'aviones', 'avisame', 'ayuda', 'ayudar', 'ayudarme', 'beber', 'beneficios', 'bien', 'buen', 'buena', 'buenas', 'bueno', 'buenos', 'buscar', 'callate', 'cambium', 'campos', 'canción', 'cargo', 'cazador', 'celebración', 'chatbot', 'chiste', 'ciencia', 'científicos', 'cita', 'claro', 'clave', 'clima', 'cocinar', 'comentario', 'cometer', 'comida', 'completar', 'computadoras', 'comunicación', 'comunicarme', 'común', 'con', 'concentrarme', 'confio', 'conoces', 'consecuencias', 'consejo', 'consejos', 'considerar', 'consiste', 'contar', 'contenido', 'contiene', 'controlar', 'cosa', 'cosas', 'creación', 'creado', 'creadores', 'crear', 'credenciales', 'creo', 'cuando', 'cuenta', 'cuentame', 'cuidar', 'cultura', 'culturales', 'cumplido', 'curiosos', 'cuéntame', 'cómo', 'da', 'dame', 'darías', 'datos', 'de', 'deberia', 'debería', 'debo', 'decir', 'del', 'demás', 'depresion', 'deprimido', 'desafío', 'descansar', 'descanso', 'descansos', 'desconectar', 'descubrimientos', 'desde', 'después', 'destino', 'detrás', 'dia', 'diario', 'dice', 'diciendo', 'dieta', 'dificultad', 'dificultades', 'dijiste', 'dime', 'dinero', 'disponibles', 'distancia', 'distracciones', 'dormido', 'dormir', 'durante', 'dónde', 'e', 'efectiva', 'efectivas', 'efectivos', 'ejercicio', 'el', 'elegiste', 'eligió', 'emocional', 'empatía', 'empezar', 'empático', 'en', 'encargada', 'encargado', 'encontrar', 'enfocandome', 'entender', 'entendí', 'entiende', 'entiendes', 'entiendo', 'entonces', 'entorno', 'entrar', 'enviar', 'equilibrado', 'equipo', 'equivocada', 'eres', 'escribir', 'escuela', 'eso', 'espacio', 'especial', 'específica', 'esta', 'estado', 'estar', 'estas', 'este', 'estilo', 'esto', 'estoy', 'estrategias', 'estresado', 'estrés', 'estudiantes', 'estudiar', 'estudio', 'estupido', 'estuvo', 'está', 'están', 'evaluaciones', 'evento', 'eventos', 'examenes', 'explica', 'explicar', 'expresar', 'fallecio', 'familia', 'favor', 'favorita', 'favorito', 'feliz', 'festival', 'figura', 'filosofía', 'filosófica', 'filósofo', 'financieros', 'finanzas', 'forma', 'forman', 'formulario', 'frase', 'fuiste', 'funciona', 'funcionan', 'fácil', 'fáciles', 'gadget', 'gatos', 'genial', 'gestionar', 'gestión', 'gracias', 'grupos', 'gusta', 'gustan', 'gustaria', 'gustas', 'gusto', 'haberme', 'habla', 'hablan', 'hablar', 'hablemos', 'hace', 'hacer', 'hago', 'hamburguesas', 'hasta', 'hay', 'hazme', 'he', 'hecho', 'hermana', 'hermano', 'hermanos', 'hice', 'hicieron', 'hmmm', 'hobby', 'hola', 'hora', 'horrible', 'hoy', 'hábitos', 'idea', 'impacto', 'importante', 'importantes', 'incluir', 'incorrecta', 'información', 'ingresar', 'iniciar', 'inicio', 'insomnio', 'inspira', 'inspiradora', 'integrantes', 'inteligencia', 'interesado', 'interesante', 'inutil', 'invertir', 'juega', 'juego', 'juegos', 'k', 'la', 'lado', 'le', 'lectura', 'leer', 'libre', 'libro', 'llama', 'llamarte', 'llevar', 'llover', 'lo', 'loco', 'logueo', 'los', 'luego', 'lugar', 'ma', 'mal', 'mama', 'manejar', 'mano', 'mantener', 'mantenerme', 'mascotas', 'matarme', 'me', 'meditacion', 'meditación', 'meditar', 'mejor', 'mejorar', 'mejoro', 'memoria', 'menciona', 'mencionaste', 'mental', 'mercado', 'merezco', 'mesa', 'mi', 'miedo', 'mismo', 'moda', 'morir', 'mostrar', 'motivacional', 'muchas', 'mucho', 'mundo', 'murio', 'más', 'mí', 'música', 'nada', 'nadie', 'necesito', 'niveles', 'no', 'noches', 'nombre', 'noticias', 'novia', 'novio', 'nuevas', 'nuevo', 'nuevos', 'número', 'o', 'objetivo', 'obligatorios', 'odias', 'odio', 'oh', 'ok', 'opinas', 'organización', 'organizado', 'organizar', 'organizarme', 'origen', 'otra', 'otro', 'padre', 'palabras', 'papa', 'para', 'parecer', 'parte', 'participar', 'participaron', 'pasa', 'pasando', 'pasatiempo', 'pasatiempos', 'pausas', 'país', 'película', 'pensado', 'pensar', 'perros', 'persona', 'pizza', 'plato', 'podrias', 'pomodoro', 'poner', 'por', 'porque', 'positiva', 'positivo', 'practicando', 'preguntarte', 'preguntas', 'preparado', 'probablemente', 'probar', 'problemas', 'profesional', 'profesores', 'proviene', 'provienen', 'proyecto', 'próximamente', 'psicológica', 'psicológico', 'psicólogo', 'puede', 'puedes', 'puedo', 'punto', 'puntos', 'página', 'que', 'quiero', 'qué', 'razon', 'realmente', 'receta', 'recetas', 'recientes', 'recomendaciones', 'recomendación', 'recomendarías', 'recomiendas', 'recordar', 'recordarme', 'reducir', 'registrarme', 'registro', 'relacion', 'relaciones', 'relajarme', 'repitiendo', 'repito', 'representa', 'responder', 'respuesta', 'resultado', 'resumen', 'reto', 'retos', 'revisó', 'revoir', 'reír', 'robot', 'rápidas', 'saber', 'sabes', 'salud', 'saludable', 'sayonara', 'se', 'sea', 'sección', 'secuencia', 'seguire', 'según', 'selectiva', 'semana', 'sencillo', 'sentido', 'sentimientos', 'sentirme', 'ser', 'seria', 'sesión', 'si', 'siento', 'significa', 'sirve', 'sirven', 'sitio', 'sobre', 'social', 'solo', 'son', 'soy', 'su', 'suena', 'sueño', 'suficiente', 'sufriendo', 'sugerencia', 'sugieres', 'suicidio', 'supongo', 'surgió', 'sé', 'tal', 'tan', 'tardes', 'tdah', 'te', 'tecnología', 'temas', 'temperatura', 'tendencias', 'tengo', 'tenido', 'terapia', 'test', 'ti', 'tiempo', 'tiene', 'tienes', 'tipo', 'toda', 'todavia', 'todo', 'tomar', 'tonto', 'trabajo', 'tradiciones', 'trata', 'tratan', 'tratar', 'triste', 'tu', 'tus', 'técnicas', 'título', 'títulos', 'ubicacion', 'ultimos', 'un', 'una', 'universidad', 'usar', 'uso', 'usuario', 'util', 'utilizo', 'vacaciones', 'vacio', 'valida', 'validar', 'validó', 'vaya', 'veo', 'ver', 'viajar', 'viaje', 'viajes', 'vida', 'video', 'videojuego', 'vista', 'vives', 'voy', 'web', 'y', 'ya', '¿como', '¿cual', '¿cuál', '¿cuáles', '¿cuándo', '¿cuántas', '¿cuánto', '¿cuántos', '¿cómo', '¿de', '¿donde', '¿dónde', '¿el', '¿en', '¿eres', '¿es', '¿hay', '¿he', '¿hice', '¿me', '¿para', '¿podemos', '¿podrias', '¿por', '¿puedes', '¿puedo', '¿que', '¿quien', '¿quién', '¿quiénes', '¿qué', '¿sobre', '¿te', '¿tienes', '¿va', 'ámbito', 'ánimo', 'últimas', 'útiles']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing and lematizing\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json', encoding='utf-8').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "# lemmatize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "pickle.dump(words,open('texts.pkl','wb'))\n",
    "pickle.dump(classes,open('labels.pkl','wb'))\n",
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el conjunto de entrenamiento\n",
    "training = []  # Asegúrate de que 'training' sea una lista\n",
    "\n",
    "for doc in documents:\n",
    "    # Inicializar nuestra bolsa de palabras\n",
    "    bag = []\n",
    "    # Lista de palabras tokenizadas para el patrón\n",
    "    pattern_words = doc[0]\n",
    "    # Lematizar cada palabra - crear la palabra base, para representar palabras relacionadas\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "\n",
    "    # Crear nuestro array de bolsa de palabras con 1 si se encuentra la palabra en el patrón actual\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # La salida es un '0' para cada etiqueta y '1' para la etiqueta actual (para cada patrón)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    # Agregar el bag y la fila de salida a la lista 'training'\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# Guardar los datos de entrenamiento\n",
    "pickle.dump(training, open('training.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "# Mezclamos nuestras características\n",
    "random.shuffle(training)\n",
    "\n",
    "# Accedemos a los patrones y a las intenciones usando un solo índice\n",
    "train_x = [item[0] for item in training]  # Patrones\n",
    "train_y = [item[1] for item in training]  # Intenciones\n",
    "\n",
    "print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos el argumento 'learning_rate' en lugar de 'lr'\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "109/109 [==============================] - 2s 4ms/step - loss: 5.0800 - accuracy: 0.0073\n",
      "Epoch 2/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 5.0547 - accuracy: 0.0165\n",
      "Epoch 3/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 5.0258 - accuracy: 0.0202\n",
      "Epoch 4/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 4.9710 - accuracy: 0.0220\n",
      "Epoch 5/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 4.9123 - accuracy: 0.0239\n",
      "Epoch 6/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 4.7596 - accuracy: 0.0495\n",
      "Epoch 7/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 4.6274 - accuracy: 0.0349\n",
      "Epoch 8/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 4.4465 - accuracy: 0.0624\n",
      "Epoch 9/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 4.3417 - accuracy: 0.0734\n",
      "Epoch 10/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 4.1283 - accuracy: 0.0844\n",
      "Epoch 11/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 3.9958 - accuracy: 0.1083\n",
      "Epoch 12/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 3.8366 - accuracy: 0.1229\n",
      "Epoch 13/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 3.6938 - accuracy: 0.1394\n",
      "Epoch 14/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 3.4710 - accuracy: 0.1578\n",
      "Epoch 15/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 3.3463 - accuracy: 0.1761\n",
      "Epoch 16/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 3.2601 - accuracy: 0.1872\n",
      "Epoch 17/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 3.0885 - accuracy: 0.2110\n",
      "Epoch 18/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.9933 - accuracy: 0.2073\n",
      "Epoch 19/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.9149 - accuracy: 0.2257\n",
      "Epoch 20/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.7900 - accuracy: 0.2495\n",
      "Epoch 21/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.6850 - accuracy: 0.3083\n",
      "Epoch 22/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.7167 - accuracy: 0.3083\n",
      "Epoch 23/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.5882 - accuracy: 0.2826\n",
      "Epoch 24/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.4932 - accuracy: 0.3339\n",
      "Epoch 25/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.4582 - accuracy: 0.3468\n",
      "Epoch 26/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.3579 - accuracy: 0.3706\n",
      "Epoch 27/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.2582 - accuracy: 0.3780\n",
      "Epoch 28/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 2.1768 - accuracy: 0.3780\n",
      "Epoch 29/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 2.1034 - accuracy: 0.4147\n",
      "Epoch 30/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.9612 - accuracy: 0.4514\n",
      "Epoch 31/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.8864 - accuracy: 0.4789\n",
      "Epoch 32/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.9955 - accuracy: 0.4330\n",
      "Epoch 33/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.8622 - accuracy: 0.4771\n",
      "Epoch 34/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.8484 - accuracy: 0.4697\n",
      "Epoch 35/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.8573 - accuracy: 0.4514\n",
      "Epoch 36/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.7787 - accuracy: 0.4807\n",
      "Epoch 37/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.8212 - accuracy: 0.4844\n",
      "Epoch 38/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.6432 - accuracy: 0.5064\n",
      "Epoch 39/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.7139 - accuracy: 0.5229\n",
      "Epoch 40/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.6905 - accuracy: 0.4844\n",
      "Epoch 41/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.6808 - accuracy: 0.5119\n",
      "Epoch 42/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.5736 - accuracy: 0.5339\n",
      "Epoch 43/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.6670 - accuracy: 0.5174\n",
      "Epoch 44/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.5680 - accuracy: 0.5505\n",
      "Epoch 45/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.4550 - accuracy: 0.5560\n",
      "Epoch 46/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.5064 - accuracy: 0.5266\n",
      "Epoch 47/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.5543 - accuracy: 0.5505\n",
      "Epoch 48/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3536 - accuracy: 0.5963\n",
      "Epoch 49/200\n",
      "109/109 [==============================] - 0s 5ms/step - loss: 1.4711 - accuracy: 0.5743\n",
      "Epoch 50/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.4097 - accuracy: 0.5890\n",
      "Epoch 51/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.4501 - accuracy: 0.5670\n",
      "Epoch 52/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.5037 - accuracy: 0.5541\n",
      "Epoch 53/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3874 - accuracy: 0.5927\n",
      "Epoch 54/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.4272 - accuracy: 0.5890\n",
      "Epoch 55/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.4621 - accuracy: 0.5743\n",
      "Epoch 56/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3995 - accuracy: 0.5908\n",
      "Epoch 57/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.4566 - accuracy: 0.5725\n",
      "Epoch 58/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.3771 - accuracy: 0.5945\n",
      "Epoch 59/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3304 - accuracy: 0.5908\n",
      "Epoch 60/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1715 - accuracy: 0.6606\n",
      "Epoch 61/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2903 - accuracy: 0.6165\n",
      "Epoch 62/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2478 - accuracy: 0.6165\n",
      "Epoch 63/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2952 - accuracy: 0.5982\n",
      "Epoch 64/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2529 - accuracy: 0.6330\n",
      "Epoch 65/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3786 - accuracy: 0.5927\n",
      "Epoch 66/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3717 - accuracy: 0.5945\n",
      "Epoch 67/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3572 - accuracy: 0.5798\n",
      "Epoch 68/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2816 - accuracy: 0.6477\n",
      "Epoch 69/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2678 - accuracy: 0.6422\n",
      "Epoch 70/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1878 - accuracy: 0.6422\n",
      "Epoch 71/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2406 - accuracy: 0.6220\n",
      "Epoch 72/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2631 - accuracy: 0.6000\n",
      "Epoch 73/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2494 - accuracy: 0.6294\n",
      "Epoch 74/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3624 - accuracy: 0.5945\n",
      "Epoch 75/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3252 - accuracy: 0.6202\n",
      "Epoch 76/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3429 - accuracy: 0.5982\n",
      "Epoch 77/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1681 - accuracy: 0.6532\n",
      "Epoch 78/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3952 - accuracy: 0.5945\n",
      "Epoch 79/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2672 - accuracy: 0.6239\n",
      "Epoch 80/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3168 - accuracy: 0.6202\n",
      "Epoch 81/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2964 - accuracy: 0.6092\n",
      "Epoch 82/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2627 - accuracy: 0.6294\n",
      "Epoch 83/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2112 - accuracy: 0.6771\n",
      "Epoch 84/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2328 - accuracy: 0.6128\n",
      "Epoch 85/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2231 - accuracy: 0.6349\n",
      "Epoch 86/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3018 - accuracy: 0.6147\n",
      "Epoch 87/200\n",
      "109/109 [==============================] - 1s 7ms/step - loss: 1.3766 - accuracy: 0.6073\n",
      "Epoch 88/200\n",
      "109/109 [==============================] - 1s 6ms/step - loss: 1.1978 - accuracy: 0.6459\n",
      "Epoch 89/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1359 - accuracy: 0.6312\n",
      "Epoch 90/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1909 - accuracy: 0.6550\n",
      "Epoch 91/200\n",
      "109/109 [==============================] - 1s 11ms/step - loss: 1.1319 - accuracy: 0.6642\n",
      "Epoch 92/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1559 - accuracy: 0.6661\n",
      "Epoch 93/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2485 - accuracy: 0.6165\n",
      "Epoch 94/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1620 - accuracy: 0.6789\n",
      "Epoch 95/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1597 - accuracy: 0.6477\n",
      "Epoch 96/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2587 - accuracy: 0.6239\n",
      "Epoch 97/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2763 - accuracy: 0.6477\n",
      "Epoch 98/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1499 - accuracy: 0.6550\n",
      "Epoch 99/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2044 - accuracy: 0.6587\n",
      "Epoch 100/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1269 - accuracy: 0.6697\n",
      "Epoch 101/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2101 - accuracy: 0.6367\n",
      "Epoch 102/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2973 - accuracy: 0.6165\n",
      "Epoch 103/200\n",
      "109/109 [==============================] - 1s 6ms/step - loss: 1.1875 - accuracy: 0.6477\n",
      "Epoch 104/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.0247 - accuracy: 0.7046\n",
      "Epoch 105/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2680 - accuracy: 0.6275\n",
      "Epoch 106/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1472 - accuracy: 0.6734\n",
      "Epoch 107/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1685 - accuracy: 0.6514\n",
      "Epoch 108/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1005 - accuracy: 0.6716\n",
      "Epoch 109/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2162 - accuracy: 0.6514\n",
      "Epoch 110/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2114 - accuracy: 0.6697\n",
      "Epoch 111/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.0637 - accuracy: 0.6661\n",
      "Epoch 112/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2412 - accuracy: 0.6514\n",
      "Epoch 113/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2798 - accuracy: 0.6257\n",
      "Epoch 114/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2437 - accuracy: 0.6422\n",
      "Epoch 115/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1262 - accuracy: 0.6862\n",
      "Epoch 116/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1622 - accuracy: 0.6459\n",
      "Epoch 117/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1124 - accuracy: 0.6716\n",
      "Epoch 118/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2134 - accuracy: 0.6642\n",
      "Epoch 119/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2127 - accuracy: 0.6532\n",
      "Epoch 120/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2450 - accuracy: 0.6789\n",
      "Epoch 121/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2120 - accuracy: 0.6514\n",
      "Epoch 122/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1405 - accuracy: 0.6789\n",
      "Epoch 123/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.0145 - accuracy: 0.7028\n",
      "Epoch 124/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.0578 - accuracy: 0.6936\n",
      "Epoch 125/200\n",
      "109/109 [==============================] - 1s 7ms/step - loss: 1.1939 - accuracy: 0.6807\n",
      "Epoch 126/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1968 - accuracy: 0.6734\n",
      "Epoch 127/200\n",
      "109/109 [==============================] - 1s 7ms/step - loss: 1.1624 - accuracy: 0.6569\n",
      "Epoch 128/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2121 - accuracy: 0.6734\n",
      "Epoch 129/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1348 - accuracy: 0.6716\n",
      "Epoch 130/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2219 - accuracy: 0.6514\n",
      "Epoch 131/200\n",
      "109/109 [==============================] - 1s 6ms/step - loss: 1.2126 - accuracy: 0.6550\n",
      "Epoch 132/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1411 - accuracy: 0.6734\n",
      "Epoch 133/200\n",
      "109/109 [==============================] - 0s 5ms/step - loss: 1.2176 - accuracy: 0.6587\n",
      "Epoch 134/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1870 - accuracy: 0.6550\n",
      "Epoch 135/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1378 - accuracy: 0.6844\n",
      "Epoch 136/200\n",
      "109/109 [==============================] - 0s 5ms/step - loss: 1.2811 - accuracy: 0.6459\n",
      "Epoch 137/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2112 - accuracy: 0.6734\n",
      "Epoch 138/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2159 - accuracy: 0.6587\n",
      "Epoch 139/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1295 - accuracy: 0.6679\n",
      "Epoch 140/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 0.9214 - accuracy: 0.7321\n",
      "Epoch 141/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3122 - accuracy: 0.6349\n",
      "Epoch 142/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3459 - accuracy: 0.6257\n",
      "Epoch 143/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0948 - accuracy: 0.6899\n",
      "Epoch 144/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2262 - accuracy: 0.6587\n",
      "Epoch 145/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2194 - accuracy: 0.6440\n",
      "Epoch 146/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2289 - accuracy: 0.6661\n",
      "Epoch 147/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1763 - accuracy: 0.6972\n",
      "Epoch 148/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0066 - accuracy: 0.6972\n",
      "Epoch 149/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0738 - accuracy: 0.6789\n",
      "Epoch 150/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1265 - accuracy: 0.6752\n",
      "Epoch 151/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0767 - accuracy: 0.6899\n",
      "Epoch 152/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2187 - accuracy: 0.6422\n",
      "Epoch 153/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0880 - accuracy: 0.6789\n",
      "Epoch 154/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0658 - accuracy: 0.6881\n",
      "Epoch 155/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2276 - accuracy: 0.6514\n",
      "Epoch 156/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1719 - accuracy: 0.6587\n",
      "Epoch 157/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.0800 - accuracy: 0.6954\n",
      "Epoch 158/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0605 - accuracy: 0.7193\n",
      "Epoch 159/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1237 - accuracy: 0.6697\n",
      "Epoch 160/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1685 - accuracy: 0.6752\n",
      "Epoch 161/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0676 - accuracy: 0.6862\n",
      "Epoch 162/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2401 - accuracy: 0.6789\n",
      "Epoch 163/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0920 - accuracy: 0.6771\n",
      "Epoch 164/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1259 - accuracy: 0.6807\n",
      "Epoch 165/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0059 - accuracy: 0.7358\n",
      "Epoch 166/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0273 - accuracy: 0.7119\n",
      "Epoch 167/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1702 - accuracy: 0.6716\n",
      "Epoch 168/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1579 - accuracy: 0.6661\n",
      "Epoch 169/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2987 - accuracy: 0.6771\n",
      "Epoch 170/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2540 - accuracy: 0.6697\n",
      "Epoch 171/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0754 - accuracy: 0.7009\n",
      "Epoch 172/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0362 - accuracy: 0.7009\n",
      "Epoch 173/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1764 - accuracy: 0.6679\n",
      "Epoch 174/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1697 - accuracy: 0.6954\n",
      "Epoch 175/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0445 - accuracy: 0.6826\n",
      "Epoch 176/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0638 - accuracy: 0.6881\n",
      "Epoch 177/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1628 - accuracy: 0.6917\n",
      "Epoch 178/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1187 - accuracy: 0.7009\n",
      "Epoch 179/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2029 - accuracy: 0.6771\n",
      "Epoch 180/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.1476 - accuracy: 0.6954\n",
      "Epoch 181/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1806 - accuracy: 0.6789\n",
      "Epoch 182/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1611 - accuracy: 0.6789\n",
      "Epoch 183/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2255 - accuracy: 0.6495\n",
      "Epoch 184/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2409 - accuracy: 0.6789\n",
      "Epoch 185/200\n",
      "109/109 [==============================] - 0s 5ms/step - loss: 1.1602 - accuracy: 0.7046\n",
      "Epoch 186/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.1396 - accuracy: 0.6826\n",
      "Epoch 187/200\n",
      "109/109 [==============================] - 0s 5ms/step - loss: 1.1103 - accuracy: 0.6771\n",
      "Epoch 188/200\n",
      "109/109 [==============================] - 0s 5ms/step - loss: 1.1857 - accuracy: 0.6661\n",
      "Epoch 189/200\n",
      "109/109 [==============================] - 0s 5ms/step - loss: 1.2147 - accuracy: 0.6862\n",
      "Epoch 190/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2439 - accuracy: 0.6495\n",
      "Epoch 191/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2523 - accuracy: 0.6587\n",
      "Epoch 192/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.0342 - accuracy: 0.6991\n",
      "Epoch 193/200\n",
      "109/109 [==============================] - 0s 5ms/step - loss: 1.0970 - accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2771 - accuracy: 0.6587\n",
      "Epoch 195/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3090 - accuracy: 0.6697\n",
      "Epoch 196/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2384 - accuracy: 0.6972\n",
      "Epoch 197/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.3141 - accuracy: 0.6514\n",
      "Epoch 198/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.2538 - accuracy: 0.6697\n",
      "Epoch 199/200\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 1.2937 - accuracy: 0.6532\n",
      "Epoch 200/200\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 1.0989 - accuracy: 0.6752\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "#fitting and saving the model\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('model.h5', hist)\n",
    "print(\"model created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
